{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project is \"Next Word Prediction\".\n",
        "\n",
        "We will build a model that can complete your sentences. This is the core technology behind features like \"Smart Compose\" in Gmail or the predictive text on your phone.\n",
        "\n",
        "We will use a Long Short-Term Memory (LSTM) network, which is excellent at remembering patterns in long sequences of text. We will train it on \"The Adventures of Sherlock Holmes\" so it learns to speak like a 19th-century detective.\n",
        "\n"
      ],
      "metadata": {
        "id": "41Bkq812n_pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Import Libraries & Load Dataset\n",
        "We will download the book text directly from Project Gutenberg."
      ],
      "metadata": {
        "id": "aXOh-ihFoEaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0ml3YfVnbRR",
        "outputId": "ccb031b2-ea44-4a27-935e-16f22e8bcc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/1661/1661-0.txt\n",
            "\u001b[1m607504/607504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2us/step\n",
            "✅ Text Loaded. Character count: 581425\n",
            "--- Sample Text ---\n",
            "gs in baker street, buried among his old\n",
            "books, and alternating from week to week between cocaine and ambition,\n",
            "the drowsiness of the drug, and the fierce energy of his own keen\n",
            "nature. he was still, as ever, deeply attracted by the study of crime,\n",
            "and occupied his immense faculties and extraordinary powers of\n",
            "observation in following out those clues, and clearing up those\n",
            "mysteries which had been abandoned as hopeless by the official police.\n",
            "from time to time i heard some vague account of his d\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# 1. Download the Dataset (Sherlock Holmes)\n",
        "path = tf.keras.utils.get_file(\n",
        "    'sherlock_holmes.txt',\n",
        "    origin='https://www.gutenberg.org/files/1661/1661-0.txt'\n",
        ")\n",
        "\n",
        "# 2. Read and Lowercase the text\n",
        "text = open(path, 'r', encoding='utf-8').read().lower()\n",
        "\n",
        "print(f\"✅ Text Loaded. Character count: {len(text)}\")\n",
        "print(\"--- Sample Text ---\")\n",
        "print(text[3000:3500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Tokenization & Sequence Creation\n",
        "Deep Learning models don't understand words; they understand numbers. We use a Tokenizer to assign a unique number to every word. Then, we create \"N-gram sequences\" (e.g., \"the cat\" -> \"sat\") to teach the model what comes next."
      ],
      "metadata": {
        "id": "YBINoVOcoHkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenize (Limit to top 2000 words for speed)\n",
        "tokenizer = Tokenizer(num_words=2000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"Dictionary size: {total_words} words\")\n",
        "\n",
        "# 2. Create Input Sequences\n",
        "# We slide a window over the text to create training samples\n",
        "# Example: \"The cat sat\" -> [The, cat], [The, cat, sat]\n",
        "input_sequences = []\n",
        "# We'll just use the first 1000 lines to keep training fast for this demo\n",
        "# (Remove [:1000] to train on the whole book if you have a GPU)\n",
        "split_text = text.split('\\n')[:2000]\n",
        "\n",
        "for line in split_text:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "print(f\"Total sequences created: {len(input_sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4mcqPgGoJw8",
        "outputId": "57256c5f-1613-4f79-ddc5-2987396d7b15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary size: 8923 words\n",
            "Total sequences created: 15419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Padding & Data Split\n",
        "Sentences have different lengths, but our model expects fixed-size inputs. We \"pad\" the shorter sequences with zeros to match the longest sentence."
      ],
      "metadata": {
        "id": "KHDtTKjwoN8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Pad Sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# 2. Split into Features (X) and Label (y)\n",
        "# X = All words EXCEPT the last one\n",
        "# y = The LAST word (which we want to predict)\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# 3. One-Hot Encode labels\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxGa_gpIoTHT",
        "outputId": "0ac52ea8-3423-4d61-9beb-58e6004cd205"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (15419, 19)\n",
            "y shape: (15419, 8923)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Build the LSTM Model\n",
        "Embedding Layer: Converts word numbers into dense vectors (captures meaning).\n",
        "\n",
        "LSTM Layer: The \"memory\" layer that understands the sequence context.\n",
        "\n",
        "Dense Layer: Outputs a probability score for every possible next word in our dictionary."
      ],
      "metadata": {
        "id": "irmMSeP4oVUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(100)) # 100 units of memory\n",
        "model.add(Dense(total_words, activation='softmax')) # Output layer\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "PggOxU6zoYAv",
        "outputId": "096f4a13-96af-4f8a-aacd-7f34083d94a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Train the Model\n",
        "We train for 50 epochs. Since we used a small slice of the book (2000 lines), this will be quick."
      ],
      "metadata": {
        "id": "6XLRe-r6ocCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training model... (This may take 1-2 minutes)\")\n",
        "history = model.fit(X, y, epochs=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajYURybyocwc",
        "outputId": "6d0064a7-f2ae-4d67-8026-8d505eac5310"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model... (This may take 1-2 minutes)\n",
            "Epoch 1/5\n",
            "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.1010 - loss: 5.5016\n",
            "Epoch 2/5\n",
            "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.1138 - loss: 5.2956\n",
            "Epoch 3/5\n",
            "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.1236 - loss: 5.1560\n",
            "Epoch 4/5\n",
            "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.1309 - loss: 5.0275\n",
            "Epoch 5/5\n",
            "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.1453 - loss: 4.9087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Test Prediction (Generate Text)\n",
        "Now for the fun part! We give it a \"seed text\" (e.g., \"Sherlock\"), and it predicts the next words one by one."
      ],
      "metadata": {
        "id": "GVJqPO33ol-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words(seed_text, next_words):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict the next word index\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "\n",
        "        # Convert index back to word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Test it out!\n",
        "print(predict_next_words(\"Sherlock Holmes\", 10))\n",
        "print(predict_next_words(\"The case was\", 10))\n",
        "print(predict_next_words(\"I am\", 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bdpZW6jonfp",
        "outputId": "d832ca1a-a292-46f7-91ed-0dc21c03282e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sherlock Holmes <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "The case was <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "I am <OOV> <OOV> <OOV> <OOV> <OOV>\n"
          ]
        }
      ]
    }
  ]
}